{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost是集成学习算法的一种，属于boosting，它的特点是序列化串行叠加基分类器，另外还有并行的bagging方法。\n",
    "> 优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。<br>\n",
    "> 缺点：对离群点敏感。<br>\n",
    "> 适用数据类型：数值型和标称型。<br>\n",
    "\n",
    "AdaBoost的一般流程：\n",
    "> 1. 收集数据:可以使用任意方法。\n",
    "> 2. 准备数据:依赖于所使用的弱分类器类型,本章使用的是单层决策树,这种分类器可以处理任何数据类型。当然也可以使用任意分类器作为弱分类器,第2章到第6章中的任一分类器都可以充当弱分类器。作为弱分类器,简单分类器的效果更好。\n",
    "> 3. 分析数据:可以使用任意方法。\n",
    "> 4. 训练算法:AdaBoost的大部分时间都用在训练上,分类器将多次在同一数据集上训练弱分类器。\n",
    "> 5. 测试算法:计算分类的错误率。\n",
    "> 6. 使用算法:同SVM一样,AdaBoost预测两个类别中的一个。如果想把它应用到多个类别的场合,那么就要像多类SVM中的做法一样对AdaBoost进行修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost运行过程如下:训练数据中的每个样本,并赋予其一个权重,这些权重构成了向量$D$。一开始,这些权重都初始化成相等值。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率,然后在同一数据集上再次训练弱分类器。在分类器的第二次训练当中,将会重新调整每个样本的权重,其中第一次分对的样本的权重将会降低,而第一次分错的样本的权重将会提高。为了从所有弱分类器中得到最终的分类结果,AdaBoost为每个分类器都分配了一个权重值alpha,这些alpha值是基于每个弱分类器的错误率进行计算的。其中,错误率ε的定义为:\n",
    "$$\\epsilon=\\frac{未正确分类的样本}{正确分类的样本}$$\n",
    "alpha的计算公式如下：\n",
    "$$\\alpha=\\frac12\\ln(\\frac{1-\\epsilon}{\\epsilon})$$\n",
    "计算出alpha值后，可以对权重向量$D$进行更新：<br>\n",
    "如果样本被正确分类，那么样本权重为：\n",
    "$$D_i^{(t+1)}=\\frac{D_i^{(t)}e^{-\\alpha}}{Sum(D)}$$\n",
    "如果样本被错误分类，那么样本权重为：\n",
    "$$D_i^{(t+1)}=\\frac{D_i^{(t)}e^{\\alpha}}{Sum(D)}$$\n",
    "计算出$D$后，AdaBoost继续迭代，不断重复训练和调整权重，直到满足停机条件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基于单层决策树构建弱分类器\n",
    "单层决策树也成为决策树桩，它仅基于单个特征来做决策，相当于决策树中的一个结点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSimpData():\n",
    "    datMat = np.matrix([[1., 2.1],\n",
    "                         [2., 1.1],\n",
    "                         [1.3, 1.],\n",
    "                         [1., 1.],\n",
    "                         [2., 1.]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat, classLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[1. , 2.1],\n",
       "         [2. , 1.1],\n",
       "         [1.3, 1. ],\n",
       "         [1. , 1. ],\n",
       "         [2. , 1. ]]), [1.0, 1.0, -1.0, -1.0, 1.0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datMat, classLabels = loadSimpData()\n",
    "datMat, classLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面第一个函数，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):\n",
    "    \"\"\"\n",
    "    将数据集按照特征进行二分类\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    threshVal : 特征的阈值\n",
    "    threshIneq : 特征小于阈值的在左结点\n",
    "    \"\"\"\n",
    "    \n",
    "    retArray = np.ones((np.shape(dataMatrix)[0], 1))\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:, dimen] <= threshVal] = -1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:, dimen] > threshVal] = 1.0\n",
    "    return retArray\n",
    "\n",
    "def buildStump(dataArr, classLabels, D):\n",
    "    \"\"\"单层决策树生成\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    dataArr : 输入样本\n",
    "    classLabels : 样本标签\n",
    "    D : 样本权重向量\n",
    "    \"\"\"\n",
    "    \n",
    "    dataMatrix = np.mat(dataArr); labelMat = np.mat(classLabels).T\n",
    "    m, n = dataMatrix.shape\n",
    "    # numSteps：在特征所有可能的值上进行遍历，对特征的取值进行切分\n",
    "    # bestStump：用于存储最佳决策树桩\n",
    "    numSteps = 10.0; bestStump = {}; bestClasEst = np.mat(np.zeros((m, 1)))\n",
    "    # 最小错误率\n",
    "    minError = np.inf\n",
    "    for i in range(n):\n",
    "        rangeMin = dataMatrix[:, i].min(); rangeMax = dataMatrix[:, i].max()\n",
    "        # 将第i个特征的取值空间切分为10份，得到步长大小\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps\n",
    "        \n",
    "        for j in range(-1, int(numSteps) + 1):\n",
    "            for inequal in ['lt', 'gt']:\n",
    "                # 设定决策特征的阈值\n",
    "                threshVal = (rangeMin + float(j) * stepSize)\n",
    "                # 获得每个样本基于第i个特征的分类结果\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)\n",
    "                errArr = np.mat(np.ones((m, 1)))\n",
    "                # 剔除所有正确分类样本\n",
    "                errArr[predictedVals == labelMat] = 0\n",
    "                # 计算误分类样本的权重\n",
    "                weightedError = D.T * errArr\n",
    "#                 print(\"split: dim %d, thresh %.2f, thresh ineqal: %s, the\\\n",
    "#                        weighted error is %.3f\" % (i, threshVal, inequal, weightedError))\n",
    "                if weightedError < minError:\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump, minError, bestClasEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'thresh': 1.3, 'ineq': 'lt'}, matrix([[0.2]]), array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = np.mat(np.ones((5, 1)) / 5)\n",
    "buildStump(datMat, classLabels, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 完整的AdaBoost算法实现\n",
    "伪代码如下：<br>\n",
    "对于每次迭代：<br>\n",
    "> 利用buildStump()函数找到最佳单层决策树<br>\n",
    "> 将最佳单层决策树加入到单层决策树数组<br>\n",
    "> 计算alpha<br>\n",
    "> 计算新的权重向量D<br>\n",
    "> 更新累计类别估计值<br>\n",
    "> 如果错误率为0.0，退出循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaBoostTrainDS(dataArr, classLabels, numIt=40):\n",
    "    \n",
    "    # 初始化弱分类器数组\n",
    "    weakClassArr = []\n",
    "    m = dataArr.shape[0]\n",
    "    D = np.mat(np.ones((m, 1)) / m)\n",
    "    aggClassEst = np.mat(np.zeros((m, 1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)\n",
    "#         print(\"D: \", D.T)\n",
    "        alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16))) # 防止除0\n",
    "        bestStump['alpha'] = alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "        # classEst是使用决策树桩得到的每个样本的分类结果相当于G(x_i)\n",
    "#         print(\"classEst: \", classEst.T)\n",
    "        expon = np.multiply(-1 * alpha * np.mat(classLabels).T, classEst)\n",
    "        # 计算权重向量。算式参考《统计学习方法》第8章\n",
    "        D = np.multiply(D, np.exp(expon))\n",
    "        D = D/D.sum()\n",
    "        # 累加前面迭代得到的分类器的输出结果\n",
    "        aggClassEst += alpha * classEst\n",
    "#         print(\"aggClassEst: \", aggClassEst.T)\n",
    "        # aggErrors是综合前面所得分类器的输出后计算得到的错误\n",
    "        aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classLabels).T, np.ones((m, 1)))\n",
    "        # 计算错误率\n",
    "        errorRate = aggErrors.sum() / m\n",
    "        print(\"total error: \", errorRate, \"\\n\")\n",
    "        if errorRate == 0.0: break\n",
    "    # 返回弱分类器数组\n",
    "    return weakClassArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:  [[0.2 0.2 0.2 0.2 0.2]]\n",
      "classEst:  [[-1.  1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "total error:  0.2 \n",
      "\n",
      "D:  [[0.5   0.125 0.125 0.125 0.125]]\n",
      "classEst:  [[ 1.  1. -1. -1. -1.]]\n",
      "aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "total error:  0.2 \n",
      "\n",
      "D:  [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]\n",
      "classEst:  [[1. 1. 1. 1. 1.]]\n",
      "aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "total error:  0.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dim': 0, 'thresh': 1.3, 'ineq': 'lt', 'alpha': 0.6931471805599453},\n",
       " {'dim': 1, 'thresh': 1.0, 'ineq': 'lt', 'alpha': 0.9729550745276565},\n",
       " {'dim': 0, 'thresh': 0.9, 'ineq': 'lt', 'alpha': 0.8958797346140273}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaBoostTrainDS(datMat, classLabels, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 测试算法：基于AdaBoost的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaClassify(datToClass, classifierArr):\n",
    "    dataMatrix = np.mat(datToClass)\n",
    "    m = dataMatrix.shape[0]\n",
    "    aggClassEst = np.mat(np.zeros((m, 1)))\n",
    "    for i in range(len(classifierArr)):\n",
    "        classEst = stumpClassify(dataMatrix, classifierArr[i]['dim'],\n",
    "                                 classifierArr[i]['thresh'], \n",
    "                                 classifierArr[i]['ineq'])\n",
    "        aggClassEst += classifierArr[i]['alpha'] * classEst\n",
    "        # print(aggClassEst)\n",
    "    return np.sign(aggClassEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:  [[0.2 0.2 0.2 0.2 0.2]]\n",
      "classEst:  [[-1.  1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "total error:  0.2 \n",
      "\n",
      "D:  [[0.5   0.125 0.125 0.125 0.125]]\n",
      "classEst:  [[ 1.  1. -1. -1. -1.]]\n",
      "aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "total error:  0.2 \n",
      "\n",
      "D:  [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]\n",
      "classEst:  [[1. 1. 1. 1. 1.]]\n",
      "aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "total error:  0.0 \n",
      "\n",
      "[[-0.69314718]]\n",
      "[[-1.66610226]]\n",
      "[[-2.56198199]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datArr, labelArr = loadSimpData()\n",
    "classifierArr = adaBoostTrainDS(datArr, labelArr, 30)\n",
    "adaClassify([0, 0], classifierArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 在一个复杂数据集上应用AdaBoost\n",
    "AdaBoost的应用：\n",
    "> 1. 收集数据:提供的文本文件。\n",
    "> 2. 准备数据:确保类别标签是+1和1而非1和0。\n",
    "> 3. 分析数据:手工检查数据。\n",
    "> 4. 训练算法:在数据上,利用 adaBoostTrainDS() 函数训练出一系列的分类器。\n",
    "> 5. 测试算法:我们拥有两个数据集。在不采用随机抽样的方法下,我们就会对AdaBoost和Logistic回归的结果进行完全对等的比较。\n",
    "> 6. 使用算法:观察该例子上的错误率。不过,也可以构建一个Web网站,让驯马师输入马的症状然后预测马是否会死去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(fileName):\n",
    "    numFeat = len(open(fileName).readline().split('\\t'))\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = []\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat - 1):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        dataMat.append(lineArr)\n",
    "        labelMat.append(float(curLine[-1]))\n",
    "    return np.array(dataMat), labelMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error:  0.36789297658862874 \n",
      "\n",
      "total error:  0.36789297658862874 \n",
      "\n",
      "total error:  0.35785953177257523 \n",
      "\n",
      "total error:  0.3511705685618729 \n",
      "\n",
      "total error:  0.3511705685618729 \n",
      "\n",
      "total error:  0.3511705685618729 \n",
      "\n",
      "total error:  0.34782608695652173 \n",
      "\n",
      "total error:  0.3511705685618729 \n",
      "\n",
      "total error:  0.3511705685618729 \n",
      "\n",
      "total error:  0.35451505016722407 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "datArr, labelArr = loadDataSet('horseColicTraining2.txt')\n",
    "classifierArray = adaBoostTrainDS(datArr, labelArr, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "testArr, testLabelArr = loadDataSet('horseColicTest2.txt')\n",
    "prediction10 = adaClassify(testArr, classifierArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errArr = np.mat(np.ones((67, 1)))\n",
    "errArr[prediction10 != np.mat(testLabelArr).T].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
